# PROJECT_NAME Test Suite

## Test Organization

```
tests/
â”œâ”€â”€ conftest.py              # Test configuration and shared fixtures
â”œâ”€â”€ smoke/                   # Quick smoke tests for basic functionality
â”‚   â””â”€â”€ test_deps.py        # Dependency availability tests
â”œâ”€â”€ unit/                    # Unit tests for individual components
â”‚   â”œâ”€â”€ test_models.py      # Data model tests
â”‚   â”œâ”€â”€ test_services.py    # Business logic tests
â”‚   â””â”€â”€ test_utils.py       # Utility function tests
â”œâ”€â”€ integration/             # Integration tests with external services
â”‚   â”œâ”€â”€ api/                # API integration tests
â”‚   â””â”€â”€ database/           # Database integration tests
â”œâ”€â”€ contract/               # API/UI contract tests
â”‚   â””â”€â”€ test_external_apis.py
â”œâ”€â”€ performance/            # Performance and load tests
â”‚   â””â”€â”€ test_benchmarks.py
â”œâ”€â”€ e2e/                    # End-to-end tests
â”‚   â””â”€â”€ test_workflows.py   # Complete user workflow tests
â””â”€â”€ helpers/               # Test helper utilities
    â””â”€â”€ fixtures.py        # Shared test fixtures
```

## Test Categories

### ğŸƒ Smoke Tests (`tests/smoke/`)
**Purpose**: Quick validation that basic functionality works
**Runtime**: < 5 seconds
**Dependencies**: None (mocked)
```bash
# Python
pytest tests/smoke/ -v

# Node.js
npm run test:smoke
```

### ğŸ”¬ Unit Tests (`tests/unit/`)
**Purpose**: Test individual components in isolation
**Runtime**: < 30 seconds total
**Dependencies**: Mocked external services
```bash
# Python
pytest tests/unit/ -v

# Node.js
npm run test:unit
```

### ğŸ”Œ Integration Tests (`tests/integration/`)
**Purpose**: Test integration with external services
**Runtime**: 30 seconds - 2 minutes
**Dependencies**: Real external services (limited calls)
```bash
# Python
pytest tests/integration/ -v -m "not slow"

# Node.js
npm run test:integration
```

### ğŸ“‹ Contract Tests (`tests/contract/`)
**Purpose**: Verify external API/UI contracts haven't changed
**Runtime**: 10-60 seconds
**Dependencies**: Internet connection
```bash
# Python
pytest tests/contract/ -v

# Node.js
npm run test:contract
```

### âš¡ Performance Tests (`tests/performance/`)
**Purpose**: Measure and validate performance metrics
**Runtime**: 1-10 minutes
**Dependencies**: Production-like environment
```bash
# Python
pytest tests/performance/ -v -m "performance"

# Node.js
npm run test:performance
```

### ğŸ­ End-to-End Tests (`tests/e2e/`)
**Purpose**: Test complete user workflows
**Runtime**: 2-10 minutes
**Dependencies**: Full application stack
```bash
# Python/Node.js with Playwright
npx playwright test

# Or using pytest
pytest tests/e2e/ -v -m "e2e"
```

## Test Markers

Use test markers to run specific test categories:

### Python (pytest)
```bash
# Quick tests only
pytest -m "smoke or unit" -v

# Skip slow tests
pytest -m "not slow" -v

# Skip tests requiring credentials
pytest -m "not credentials" -v

# Integration tests only  
pytest -m "integration" -v

# Performance tests only
pytest -m "performance" -v

# End-to-end tests only
pytest -m "e2e" -v
```

### Node.js (npm scripts)
```bash
# Quick tests
npm run test:quick

# All tests except slow
npm run test:fast

# Integration tests
npm run test:integration

# E2E tests
npm run test:e2e
```

## Ops CLI Integration

Use the ops CLI for standardized testing across all projects:

```bash
# Run all quality checks (includes tests)
./scripts/ops qa

# Run tests specifically
./scripts/ops qa --tests-only

# Run fast tests only
./scripts/ops qa --fast-only

# Check test coverage
./scripts/ops qa --coverage
```

## Environment Variables

Set these for comprehensive testing:

```bash
# Application-specific credentials
export APP_API_KEY="your-test-api-key"
export APP_DATABASE_URL="your-test-db-url"

# Third-party service credentials
export EXTERNAL_SERVICE_KEY="your-service-key"

# Test environment settings
export NODE_ENV="test"
export PYTHON_ENV="test"
export CI="true"  # For CI/CD environments
```

## Test Configuration Files

### Python Projects
- `conftest.py` - Pytest configuration and fixtures
- `pytest.ini` or `pyproject.toml` - Test settings and markers
- `.coveragerc` - Coverage configuration

### Node.js Projects
- `jest.config.js` - Jest configuration
- `playwright.config.ts` - E2E test configuration
- `vitest.config.ts` - Vite test configuration (if using Vitest)

## CI/CD Integration

### GitHub Actions
```yaml
- name: Run Tests
  run: |
    ./scripts/ops qa
    
- name: Upload Coverage
  uses: codecov/codecov-action@v3
  with:
    file: ./coverage.xml
```

### Quality Gates
All tests must pass before:
- Merging pull requests
- Creating releases
- Deploying to production

## Best Practices

### Test Organization
- **One test file per source file** when possible
- **Group related tests** in test classes/describe blocks
- **Use descriptive test names** that explain the scenario
- **Keep tests independent** - no dependencies between tests

### Test Data
- **Use fixtures** for complex test data setup
- **Mock external services** in unit tests
- **Use test databases** for integration tests
- **Clean up after tests** to avoid side effects

### Performance Considerations
- **Mark slow tests** with appropriate markers
- **Run fast tests in CI** for quick feedback
- **Schedule performance tests** separately
- **Monitor test execution time** and optimize

### Documentation
- **Document complex test scenarios**
- **Explain test data requirements**
- **Provide setup instructions** for new contributors
- **Keep test documentation current**

## Troubleshooting

### Common Issues

**Tests failing in CI but passing locally:**
```bash
# Check environment differences
./scripts/ops env doctor

# Run tests in CI-like environment
CI=true ./scripts/ops qa
```

**Slow test execution:**
```bash
# Run only fast tests
./scripts/ops qa --fast-only

# Profile test execution
pytest --durations=10
```

**Flaky tests:**
```bash
# Run tests multiple times
pytest --count=10 tests/integration/

# Check for race conditions
pytest -x --tb=short
```

### Getting Help

1. Check test logs: `./scripts/ops qa --verbose`
2. Review test configuration files
3. Check environment variables
4. Run individual test files to isolate issues
5. Consult project-specific testing documentation

This standardized testing structure ensures consistent, reliable testing across all projects while maintaining flexibility for project-specific needs.